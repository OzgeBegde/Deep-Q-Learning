# -*- coding: utf-8 -*-
"""DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fD5QLOW-HUK7s3y_qNDdL6BOUxY4GAjr
"""

import numpy as np
from collections import namedtuple
from random import randint

class DQN():
 
  def __init__(self, sensor_num, hidden_layer_size,action_num,w1_pre=None,w2_pre=None):
         
        if w1_pre == None:
            self.w1 = np.random.rand(sensor_num, hidden_layer_size)
            self.w2 = np.random.rand(hidden_layer_size, action_num)
        else:
            self.w1 = w1_pre
            self.w2 = w2_pre
        
        self.dim_hidden = 15
        self.dim_action = 3
        self.dim_input = 5

        self.state=np.zeros(5)
        
        
        self.learning_rate = 0.01    # Experiment with the learning rate
        self.discount_rate = 0.3    # Should this gradually drop?
        self.gamma = 0.9
        
        self.q_est = 0


  def calculate_reward(self,state):
    total_reward = 0
    reward_set = [-100, 5, 15, 25]     # Experiment on the crash state value.
    for s in range(len(state)):
        if s < 3:
            total_reward += reward_set[state[s]+1]
        else:
            if state[s] == 2:
                total_reward += 25
            elif state[s] == 1:
                total_reward += 15
            else:
                total_reward += -20
    return total_reward

  def nn_forward(self,state):
    L2=(np.exp(np.dot(state,self.w1)*2) -1) / (np.exp(np.dot(state,self.w1)*2) +1)
    Q=(np.exp(np.dot(L2,self.w2)*2) -1) / (np.exp(np.dot(L2,self.w2)*2) +1)
    return Q

  def computeQEstimate(self, state):
    reward_temp=self.calculate_reward(state)
    q_next = self.nn_forward(state)
    q_est_value = reward_temp + self.gamma*np.max(q_next)
    q_est_index=q_next.argmax()
    return q_est_value,q_est_index
    

##  def Q_next(self,state):
##    reward_temp=self.calculate_reward(state)
##    q_next= reward_temp+ self.gamma*np.max(self.nn_forward(state))
##    return q_next
    


  def Q_est(self,state):
    reward_temp=self.calculate_reward(state)
    self.q_est= reward_temp+ self.gamma*np.max(self.nn_forward(state))
    return self.q_est

  def Back_prog(self,state):
    state = np.array(state)
    q_est_value,q_est_index = self.computeQEstimate(state)
    C=0.5*((self.q_est-q_est_value)**2)    
    delta_3 = (self.nn_forward(state)[q_est_index]-self.q_est)*np.identity(self.dim_action)[:,q_est_index]
    delta_2=np.multiply(np.dot(self.w2,delta_3),((4*np.cosh(np.dot(state,self.w1))**2)/((1+np.cosh((np.dot(state,self.w1))*2))**2)))    
    w1_grad = np.dot(delta_2.reshape(self.dim_hidden,1),state.reshape(1,self.dim_input)).transpose()    
    w2_grad = np.dot(delta_3.reshape(self.dim_action,1),((np.exp(np.dot(state.transpose(),self.w1)*2) -1) / (np.exp(np.dot(state.transpose(),self.w1)*2) +1)).reshape(1,self.dim_hidden)).transpose()
    return w1_grad,w2_grad

  def Grad_des(self,state):
    w1_grad,w2_grad=self.Back_prog(state)
    self.w1 = self.w1 - self.learning_rate*w1_grad
    self.w2 = self.w2 - self.learning_rate*w2_grad
    return self.w1,self.w2

  def get_best_action(self, state):
##    temp_reward= calculate_reward(state) 
    temp = self.Q_est(state)       
    max_value = np.max(temp)
    max_value_index =temp.argmax()
#### QQ[self.max_value_index]= self.max_value
    return int(max_value_index)
    


  def get_random_action(self):
    return randint(0, 2)




def save_table(dqn):
    np.savetxt("Saved-W1-Values.txt", dqn.w1)
    np.savetxt("Saved-W2-Values.txt", dqn.w2)

def load_table():
    return np.loadtxt("Saved-W1-Values.txt"),np.loadtxt("Saved-W2-Values.txt")

